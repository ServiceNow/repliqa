{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating HF models fine-tuned on QA on RepLiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider using the optional_requirements.txt file to set up an environment to run this notebook.\n",
    "\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get repliqa and its set of topics\n",
    "Here we're only using the `repliqa_0` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repliqa = datasets.load_dataset(\"ServiceNow/repliqa\")[\"repliqa_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineQAEvaluator:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        cache_path: str = None,\n",
    "        max_length: int = 10_000,\n",
    "    ) -> None:\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            cache_dir=cache_path,\n",
    "        ).eval()\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "        self.terminators = [\n",
    "            self.tokenizer.eos_token_id,\n",
    "            self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"), # As per 'How to use' in https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B\n",
    "        ]\n",
    "        self.max_length = max_length\n",
    "        self.device = next(self.model.parameters()).device\n",
    "\n",
    "    def __call__(self, example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "        tokenized_inputs = self._make_inputs(example)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            outputs = self.model.generate(\n",
    "                **tokenized_inputs,\n",
    "                max_new_tokens=128,\n",
    "                eos_token_id=self.terminators,\n",
    "            )\n",
    "\n",
    "        response = outputs[0][tokenized_inputs[\"input_ids\"].shape[-1] :]\n",
    "\n",
    "        raw_response = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        return {\n",
    "            \"answer_pred\": raw_response,\n",
    "            \"cleaned_answer_pred\": self.process_answer(raw_response),\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "\n",
    "    def process_answer(self, raw_answer: str) -> str:\n",
    "        return raw_answer.split(\":\")[-1]\n",
    "\n",
    "    def _make_inputs(self, example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if \"document_extracted\" in example:\n",
    "            context_str = example[\"document_extracted\"]\n",
    "        elif \"entity_pages\":\n",
    "            context_str = example[\"entity_pages\"][\"wiki_context\"][0]\n",
    "\n",
    "        question_str = example[\"question\"]\n",
    "\n",
    "        # We followed the prompt format recommended in:\n",
    "        # https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B\n",
    "        input_str = (\n",
    "            \"System: This is a chat between a user and an artificial intelligence assistant. \"\n",
    "            \"The assistant gives helpful, detailed, and polite answers to the user's \"\n",
    "            \"questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "            \"concisely and precisely, using information provided by the user.\"\n",
    "            f\"\\n\\n{context_str}\"\n",
    "            f\"\\n\\nUser: Please give a full and complete answer for the question. {question_str}\"\n",
    "            \"\\n\\nAssistant: \"\n",
    "        )\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "                [self.tokenizer.bos_token + input_str],\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "            ).to(self.device)\n",
    "\n",
    "        return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values in 'models' should be either 'nvidia/Llama3-ChatQA-1.5-8B or https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B'\n",
    "models = [\"nvidia/Llama3-ChatQA-1.5-8B\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_datasets = {}\n",
    "\n",
    "for model in tqdm.tqdm(models, total=len(models), desc=\"Models\"):\n",
    "  qa_pre_processor = BaselineQAEvaluator(model_name=model)\n",
    "  prediction_dataset = repliqa.map(qa_pre_processor)\n",
    "\n",
    "  inference_datasets[model] = prediction_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
